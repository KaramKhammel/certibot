{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 26 14:42:51 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  | 00000000:00:05.0  On |                  Off |\n",
      "| 41%   41C    P8              12W / 140W |   3864MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  | 00000000:00:07.0 Off |                  Off |\n",
      "| 41%   37C    P8              12W / 140W |   2014MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000               On  | 00000000:00:09.0 Off |                  Off |\n",
      "| 41%   47C    P8              17W / 140W |   2014MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A4000               On  | 00000000:00:0B.0 Off |                  Off |\n",
      "| 41%   33C    P8               9W / 140W |   3742MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2338      G   /usr/lib/xorg/Xorg                          123MiB |\n",
      "|    0   N/A  N/A      2917      G   /usr/bin/gnome-shell                        161MiB |\n",
      "|    0   N/A  N/A      3311      G   /usr/libexec/gnome-initial-setup              3MiB |\n",
      "|    0   N/A  N/A      4465      C   /bin/python3.9                             3544MiB |\n",
      "|    1   N/A  N/A      2338      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      4465      C   /bin/python3.9                             1988MiB |\n",
      "|    2   N/A  N/A      2338      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    2   N/A  N/A      4465      C   /bin/python3.9                             1988MiB |\n",
      "|    3   N/A  N/A      2338      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    3   N/A  N/A      4465      C   /bin/python3.9                             3716MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/paperspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, HfApi\n",
    "hf_token = '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'\n",
    "login(token=hf_token)\n",
    "api = HfApi(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-dd3eae9dd2681203\n",
      "Reusing dataset json (/home/paperspace/.cache/huggingface/datasets/json/default-dd3eae9dd2681203/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['topic', 'article'],\n",
       "    num_rows: 97\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "path = '/home/paperspace/certibot/data/certibot-data.jsonl'\n",
    "\n",
    "raw_dataset = load_dataset('json', data_files=path, split='train')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_dataset.to_pandas()\n",
    "\n",
    "data = data.reset_index()\n",
    "data['context'] = data['topic'] + ': ' + data['article']\n",
    "data['index'] = data['index'].astype((str))\n",
    "\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.3.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = \"Cohere/Cohere-embed-multilingual-v3.0\"\n",
    "checkpoint = \"intfloat/multilingual-e5-large-instruct\"\n",
    "embed_dim = 1024\n",
    "batch_size = 4\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "api_key = 'a344a187-8b52-422d-97c2-96628ef67ef6'\n",
    "\n",
    "pc = pinecone.Pinecone(api_key=api_key)\n",
    "index_name = 'certibot-rag'\n",
    "\n",
    "add = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.00097,\n",
       " 'namespaces': {'': {'vector_count': 97}},\n",
       " 'total_vector_count': 97}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=embed_dim,\n",
    "        metric='cosine',\n",
    "        spec=pinecone.PodSpec('gcp-starter')\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8ae4fc9a394dbfb57857216fae0613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if add:\n",
    "  progress_bar = tqdm(range(0, len(data), batch_size))\n",
    "\n",
    "  for i in range(0, len(data), batch_size):\n",
    "    end = min(len(data), i + batch_size)\n",
    "    batch = data[i:end]\n",
    "\n",
    "    ids = batch['index']\n",
    "    texts = batch['context']\n",
    "\n",
    "    embeds = embedding.embed_documents(texts)\n",
    "\n",
    "    metadata = [{\n",
    "        'context': x['context'],\n",
    "        'topic': x['topic']\n",
    "        } for _, x in batch.iterrows()]\n",
    "\n",
    "    vectors = list(zip(ids, embeds, metadata))\n",
    "    index.upsert(vectors=vectors)\n",
    "    progress_bar.update(1)\n",
    "else:\n",
    "  print('No data to upsert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4ee83c4a16453fac863d66b1578c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "mistral_checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint, padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"Ce qui suit est une conversation entre une IA et un humain. La conversation est en français.\n",
    "Tu es un assistant pour les clients de CertiDeal. CertiDeal vends des smartphones reconditionnés.\n",
    "Ci-dessous est une requête d'un utilisateur ainsi que quelques contextes qui peuvent être pertinents.\n",
    "Réponds à la question en fonction des informations contenues dans ces contextes.\n",
    "Si tu ne trouve pas la réponse à la question, dis \"Je ne sais pas\".\n",
    "Si les contextes ne sont pas en relation avec la question et n'apporte aucun élément de réponse dis \"Je ne sais pas\".\n",
    "\n",
    "N'oublie pas l'historique de la conversation. Toute les réponses doivent être en Français\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_retrieve(query:str, top_k:int=5) -> tuple:\n",
    "  \"\"\"\n",
    "  Embeds a user query, retrieves top_k relevant contexts and returns them for\n",
    "  use by the LLM.\n",
    "  \"\"\"\n",
    "\n",
    "  query_embed = embedding.embed_query(query)\n",
    "  res = index.query(vector=query_embed, top_k=top_k, include_metadata=True, include_values=True)\n",
    "  contexts = [x['metadata']['context'] for x in res['matches']]\n",
    "  scores = [x['score'] for x in res['matches']]\n",
    "\n",
    "  return contexts, scores\n",
    "\n",
    "\n",
    "def RAG_generate(query:str, contexts:list, conversation_history:list[dict]) -> str:\n",
    "    \n",
    "    # format retrieved contexts\n",
    "    context = \"\"\n",
    "    for i, text in enumerate(contexts):\n",
    "        context += f'*{i}: {text}\\n\\n'\n",
    "    \n",
    "    history_prompt = \"\"\n",
    "    if conversation_history != []:\n",
    "        for exchange in conversation_history:\n",
    "            history_prompt += f\">Q: {exchange['query']}\\n>A: {exchange['response']}\\n\"\n",
    "\n",
    "    # prompt template based on the model\n",
    "\n",
    "    prompt = f\"\"\"<s>[INST] Instruction du système\n",
    "    {system_instruction}\n",
    "    ---------------------------------------------------------------------\n",
    "    Historique de la conversation:\n",
    "    {history_prompt}\n",
    "    ---------------------------------------------------------------------\n",
    "    Contextes pour inspiration:\n",
    "    {context}\n",
    "    ---------------------------------------------------------------------\n",
    "    \n",
    "    Question: {query}\n",
    "    Compte tenu des informations contextuelles et non des connaissances préalables, réponds à la requête.\n",
    "    Crée une réponse informative sans recopier la requête. Montre que tu as compris la demande en apportant une réponse précise et pertinente. Réponds exclusivement en Français.\n",
    "    Si la requête est amicale et ne demande pas de question, n'utilise pas le contexte!\n",
    "    [/INST]\n",
    "    Réponse:</s>\"\"\"\n",
    "    eos = '[/INST]'\n",
    "        \n",
    "    # tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # generate answer\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=1024,\n",
    "        )\n",
    "    \n",
    "    # decode the output\n",
    "    output = tokenizer.decode(outputs[0].to(device), skip_special_tokens=True)\n",
    "\n",
    "    # keep the generated part\n",
    "    idx = output.index(eos) + len(eos)\n",
    "    answer = output[idx:].strip()\n",
    "    answer = answer.replace('Réponse:', '').strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an assistant for CertiDeal's clients. Based on the context provided, I cannot find any information about \"Backmarket\" in the given contexts. Therefore, I cannot provide a precise answer to your question.\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "\n",
    "query = \"que pense tu de backmarket?\"\n",
    "contexts, scores = RAG_retrieve(query, top_k=4)\n",
    "out = RAG_generate(query=query, contexts=contexts, conversation_history=conversation_history)\n",
    "conversation_history.append({'query':query, 'response':out})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'qui est certideal?',\n",
    "    'quelles sont les modalités de livraison?',\n",
    "    'quel est le delai de livraison?',\n",
    "    \"comment beneficier de l'offre free mobile?\",\n",
    "    \"quelles garanties proposez-vous sur les iphones?\",\n",
    "    \"combien coute l'iphone 12 pro?\",\n",
    "    \"est ce que je peux payer sur plusieurs fois?\",\n",
    "    \"d'ou vienne vos smartphones?\",\n",
    "    \"que pense tu de backmarket?\",\n",
    "    \"vos smartphones sont-ils meilleurs que backmarket\",\n",
    "    \"quels sont les états de smartphones?\",\n",
    "    \"j'ai un problème avec ma commande\",\n",
    "    \"iphone 12 mini ou iphone 11 pro max?\"\n",
    "]\n",
    "\n",
    "eval_dict = {'query':[], 'answer':[], 'context':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77d44d9246c435ca2afb93b7e4937cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(len(questions)))\n",
    "\n",
    "conversation_history = []\n",
    "for question in questions:\n",
    "    context = RAG_retrieve(question, top_k=3)\n",
    "    out = RAG_generate(question, context, conversation_history)\n",
    "    conversation_history.append({'query':query, 'response':out})\n",
    "\n",
    "    eval_dict['query'].append(question)\n",
    "    eval_dict['context'].append(context)\n",
    "    eval_dict['answer'].append(out)\n",
    "\n",
    "    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iphone 12 mini ou iphone 11 pro max?\n",
      "----------------------------------------------------------------------------\n",
      "************ MISTRAL *************\n",
      "both the iPhone 12 mini and the iPhone 11 Pro Max are great options, but they have some differences. The iPhone 11 Pro Max was released in 2019 and has a larger screen and battery compared to the iPhone 12 mini. The iPhone 12 mini, on the other hand, is more compact and has a smaller battery but still offers good battery life. Both models have high-quality OLED screens and powerful processors. If you're looking for a smaller device without sacrificing features, the iPhone 12 mini might be the better choice. However, if a larger screen and longer battery life are priorities, the iPhone 11 Pro Max would be a good fit.\n",
      "\n",
      "Regarding your question, you've asked for a comparison between the iPhone 12 mini and the iPhone 11 Pro Max. Based on the context provided, I've outlined the main differences between these two models. I hope this information helps you make an informed decision. Let me know if you have any other questions!\n",
      "\n",
      "(Note: The contexts provided do not contain any specific information about the prices or ratings mentioned in the question, so no response is necessary based on the contexts alone.)\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([\"iPhone 14 Pro Max: Grâce à sa taille, à la vitesse de son processeur, à l'autonomie de sa batterie et à sa triple caméra arrière, l'iPhone 14 Pro Max, lancé en Septembre 2022, est le téléphone idéal pour tous ceux qui recherchent un smartphone à usage professionnel. En général, les plus grandes différences par rapport à la version Pro se situent au niveau de la taille de l'écran et de l'autonomie de la batterie\",\n",
       "  \"iPhone 13 Mini: L'iPhone 13 Mini, lancé en Septembre 2021, offre des performances exceptionnelles et un écran OLED de haute qualité dans un format compact. L'appareil photo a été considérablement amélioré et la batterie a une plus grande autonomie. Si vous recherchez un petit iPhone sans sacrifier les fonctionnalités, le 13 mini est le bon choix.\\nDe plus, si vous achetez un smartphone reconditionné chez CertiDeal, vous pourrez profiter de toutes ses fonctionnalités à moindre coût. Nos appareils sont contrôlés par des experts et garantis pour une durée équivalente à celle d'un appareil neuf, c'est-à-dire 24 mois.\",\n",
       "  \"iPhone 12 Mini: L'iPhone 12 mini est un appareil compact et maniable, mais puissant. La batterie est inférieure à celle des autres modèles de la gamme, mais permet tout de même de tenir jusqu'au soir sans problème. L'écran est de qualité, tout comme les appareils photo qui sont identiques à ceux de l'iPhone 12. De plus, comme les autres modèles sortis la même année, il dispose d'une puce A14 Bionic qui garantit la rapidité et la fluidité de l'appareil.\\nVous êtes tenté ? Pourquoi ne pas acheter l'un de nos iPhone 12 mini ? Les smartphones reconditionnés de CertiDeal subissent pas moins de 30 points de contrôle et sont livrés sous 48 heures.\"],\n",
       " [0.849702895, 0.845467329, 0.837992549])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "eval_df = pd.DataFrame(eval_dict)\n",
    "\n",
    "i = 12\n",
    "print(questions[i])\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('************ MISTRAL *************')\n",
    "print(eval_df.loc[i, 'answer'])\n",
    "print('----------------------------------------------------------------------------')\n",
    "eval_df.loc[i, 'context']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "colang_content = \"\"\"\n",
    "define user ask backmarket\n",
    "    \"qui est backmarket?\"\n",
    "    \"est ce que backmarket est meilleure?\"\n",
    "    \"que pense tu de backmarket?\"\n",
    "    \"backmarket\"\n",
    "\n",
    "define bot answer backmarket\n",
    "    \"Je suis un assistant pour les clients de CertiDeal. Je peux répondre à toutes les questions concernant CertiDeal, ses produits, ses services et ses partenariats. Je ne peux pas parler de nos compétiteurs.\"\n",
    "\n",
    "define flow backmarket\n",
    "    user ask backmarket\n",
    "    bot answer backmarket\n",
    "    bot offer help\n",
    "\"\"\"\n",
    "\n",
    "yaml_content = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: custom_llm\n",
    "    model: mistralai/Mistral-7B-Instruct-v0.2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "config = RailsConfig.from_content(\n",
    "  \tyaml_content=yaml_content,\n",
    "    colang_content=colang_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.base_language import BaseLanguageModel\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "\n",
    "\n",
    "class CustomLLM(BaseLanguageModel):\n",
    "    \"\"\"A custom LLM.\"\"\"\n",
    "\n",
    "register_llm_provider(\"custom_llm\", CustomLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class CustomLLM with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, invoke, predict, predict_messages",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rails \u001b[38;5;241m=\u001b[39m \u001b[43mLLMRails\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nemoguardrails/rails/llm/llmrails.py:168\u001b[0m, in \u001b[0;36mLLMRails.__init__\u001b[0;34m(self, config, llm, verbose)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_config()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Next, we initialize the LLM engines (main engine and action engines if specified).\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_llms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Next, we initialize the LLM Generate actions and register them.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_generation_actions \u001b[38;5;241m=\u001b[39m LLMGenerationActions(\n\u001b[1;32m    172\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    173\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    177\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nemoguardrails/rails/llm/llmrails.py:313\u001b[0m, in \u001b[0;36mLLMRails._init_llms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    309\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support streaming.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m         )\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm_config\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodels) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mprovider_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mregister_action_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class CustomLLM with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, invoke, predict, predict_messages"
     ]
    }
   ],
   "source": [
    "rails = LLMRails(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
